{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "- Creates embeddings (transforms unstructured texts into vectors of numbers based on pre-trained LLM)\n",
    "- Inspired by [BERTopic documentation](https://maartengr.github.io/BERTopic/index.html)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change the code so i'm working on one dataset with an extra variable: Referential (boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = \"/data/df.xlsx\"  # the documents we want to embed must be in their own rows\n",
    "df = pd.read_excel(data)\n",
    "\n",
    "# inspect df\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How long are the documents we will embed? \n",
    "- We should check, as BERT has a limit of 512 tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick visualization to see text lengths\n",
    "ax = l.hist(bins=30)  # create histogram using axes object\n",
    "ax.set_title('How many \"tokens\" long are control texts?')\n",
    "plt.axvline(x=512, color='red', linewidth=1)\n",
    "props = dict(boxstyle='square', facecolor='white', alpha=0.5)\n",
    "\n",
    "ax.text(0.95, 0.7, f'{z} values >= 512', transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top', horizontalalignment='right', bbox=props)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many are too long?\n",
    "l = df['full_control_text'].apply(lambda x: len(x.split(' ')))\n",
    "z = len([x for x in l if x >= 512])\n",
    "print(f'{z} values are over length 512 and will be truncated.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create embeddings, we need a list of documents (control texts in our case)\n",
    "docs = list(df['full_control_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"  \n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to do BERT embedding on text\n",
    "def get_embedding(text):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)  # truncates\n",
    "    output = model(**encoded_input)\n",
    "    \n",
    "    # Use the pooler output (representation of [CLS] token) and detach it to convert tensor to numpy\n",
    "    embedding = output[1].detach().numpy()\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column in our dataset for embeddings\n",
    "df['BERTembeddings'] = df['full_control_text'].apply(get_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuned Model\n",
    "\n",
    "- Let's try the same thing - but using our finetuned version of BERT. \n",
    "- Again, we'll just add these new embeddings as a new column.\n",
    "- That lets us easiliy compare results if we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load finetuned model \n",
    "model_name = \"bert-base-uncased\"  \n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Make tokenizer based on the model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned sentence-transformers model\n",
    "model_path = \"outputs/sentence_transformers_compatible_model\"\n",
    "finetuned_model = SentenceTransformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each document\n",
    "embeddings = finetuned_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# Add the embeddings to DataFrame\n",
    "df['finetune_embeddings'] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling\n",
    "\n",
    "Having created the embeddings, we can do most of the rest of the BERTopic algorithm in one function\n",
    "<!-- - Dimensionality reduction\n",
    "- Clustering\n",
    "- Tokenizer\n",
    "- Weighting Scheme -->\n",
    "\n",
    "\n",
    "- Step 1 - Extract embeddings\n",
    "\n",
    "` embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")`\n",
    "\n",
    "- Step 2 - Reduce dimensionality\n",
    "\n",
    "`umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')`\n",
    "\n",
    "- Step 3 - Cluster reduced embeddings\n",
    "\n",
    "`hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)`\n",
    "\n",
    "- Step 4 - Tokenize topics\n",
    "\n",
    "`vectorizer_model = CountVectorizer(stop_words=\"english\")`\n",
    "\n",
    "- Step 5 - Create topic representation\n",
    "\n",
    "`ctfidf_model = ClassTfidfTransformer()`\n",
    "\n",
    "- Step 6 - (Optional) Fine-tune topic representations \n",
    "\n",
    "`representation_model=representation_model` \n",
    "\n",
    "See [documentation](https://maartengr.github.io/BERTopic/getting_started/parameter%20tuning/parametertuning.html#min_topic_size \"More info on minimum topic size and other parameters\") for more on min_topic_size and other parameter choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic modeling function\n",
    "def topic_modeling(min_topic_size):\n",
    "    # Load fine-tuned sentence-transformers model\n",
    "    finetuned_model = SentenceTransformer(r\"outputs\\fine_tuned_model\")\n",
    "    # Load pre-generated embeddings\n",
    "    pre_generated_embeddings = np.array(list(df['finetune_embeddings'].values))\n",
    "    # specifying dimensionality reduction\n",
    "    umap_model = UMAP.UMAP(n_neighbors=15, n_components=5, metric='cosine', low_memory=False)  # may need to tweak\n",
    "    # specifying cluster model  - requires setting minimum topic size ()\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', prediction_data=True) \n",
    "    # Create two representation models (maybe one is better - nice to check)\n",
    "    keybert_model = KeyBERTInspired(random_state=42)\n",
    "    mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "    representation_model = {\n",
    "        \"KeyBERT\": keybert_model,\n",
    "        \"MMR\": mmr_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate BERTopic with fine-tuned model's embeddings and the representation model\n",
    "topic_model = BERTopic(embedding_model='_____________',  \n",
    "                    verbose=False,\n",
    "                    n_gram_range=(1, 3),  # we use n-grams of 1-3 words\n",
    "                    min_topic_size=min_topic_size,\n",
    "                    calculate_probabilities=True,\n",
    "                    representation_model=representation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the topic modeling\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings=pre_generated_embeddings)\n",
    "    return topics, probs, topic_model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
